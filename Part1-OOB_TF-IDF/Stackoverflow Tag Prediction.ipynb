{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b><center><b>Predicting Tags </b></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our objective is to predict tags for posts from StackOverflow using Linear Model after carefully preprocessing our text features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our dataset consists of post titles from StackOverflow. We have the dataset in 3 parts(set of 3): train, validation and test. All corpora (except for test) contain titles of the posts and corresponding tags (100 tags are available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: We will also demonstrate how to create small test/dummy samples/functions to check that our main and helper functions are working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important libraries needed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import collections \n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "from scipy import sparse as sp_sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Here we will need to use a list of stop words. We can be downloaded from nltk and import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Upload the corpora using *pandas* and look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('data/train.tsv')\n",
    "validation = read_data('data/validation.tsv')\n",
    "test = pd.read_csv('data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving our dataset into train,validation and test sets.\n",
    " - *train* data for training the model;\n",
    " - *validation* data for evaluation and hyperparameters tuning;\n",
    " - *test* data for final evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to draw a stacked dotplot in R?</td>\n",
       "      <td>['r']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mysql select all records where a datetime fiel...</td>\n",
       "      <td>['php', 'mysql']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to terminate windows phone 8.1 app</td>\n",
       "      <td>['c#']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get current time in a specific country via jquery</td>\n",
       "      <td>['javascript', 'jquery']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Configuring Tomcat to Use SSL</td>\n",
       "      <td>['java']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                      tags\n",
       "0                How to draw a stacked dotplot in R?                     ['r']\n",
       "1  mysql select all records where a datetime fiel...          ['php', 'mysql']\n",
       "2             How to terminate windows phone 8.1 app                    ['c#']\n",
       "3  get current time in a specific country via jquery  ['javascript', 'jquery']\n",
       "4                      Configuring Tomcat to Use SSL                  ['java']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see, 'title' column contains titles of the posts and 'tags' column contains the tags. It could be noticed that a number of tags for a post is not fixed and could be as many as necessary.\n",
    "\n",
    "#### We will further split our train, validation and test set into title and tags separately and and initialize as X_train, X_val, X_test, y_train, y_val,y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_val, y_val = validation['title'].values, validation['tags'].values\n",
    "X_test = test['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How to draw a stacked dotplot in R?'\n",
      " 'mysql select all records where a datetime field is less than a specified value'\n",
      " 'How to terminate windows phone 8.1 app']\n",
      "[\"['r']\" \"['php', 'mysql']\" \"['c#']\"]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3])\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One of the most known difficulties when working with natural data is that it's unstructured. For example, if you use it \"as is\" and extract tokens just by splitting the titles by whitespaces, you will see that there are many weird tokens like **%^^3!@, *\"Flip*, @@{SQL}, ComPAct etc. To prevent the problems, it's usually useful to prepare the data somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will implement a function 'text_prepare' which will help in cleaning and giving our natural data some structure by converting all text to lowercase, removing stop words and bad symbols."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]') \n",
    "\n",
    "# take all words that contain characters other than 0-9,a-z,#,+,_ \n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]') \n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    #text = # lowercase text\n",
    "    text = text.lower()\n",
    "    #text = # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text)\n",
    "    #text = # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text =  re.sub(BAD_SYMBOLS_RE, '', text)\n",
    "    #text = # delete stopwords from text\n",
    "    \n",
    "    token_word = word_tokenize(text)\n",
    "    \n",
    "    # filtered_sentence contain all words that are not in stopwords dictionary\n",
    "    filtered_sentence = [w for w in token_word if not w in STOPWORDS] \n",
    "    \n",
    "    lenght_of_string = len(filtered_sentence)\n",
    "    text_new = \"\"\n",
    "    for w in filtered_sentence:\n",
    "        if w != filtered_sentence[lenght_of_string - 1]:\n",
    "             text_new = text_new + w + \" \" # when w is not the last word so separate by whitespace\n",
    "        else:\n",
    "            text_new = text_new + w\n",
    "            \n",
    "    text = text_new\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific symbols that we would want to replace with spaces\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "\n",
    "# delete unknown symbols by taking all words that contain characters other than 0-9,a-z,#,+,_ \n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "STOPWORDS = list((stopwords.words('english')))\n",
    "\n",
    "def text_prepare(text,join_symbol):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    # lowercase text\n",
    "    text = text.lower() \n",
    "\n",
    "    # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE,\" \",text,)\n",
    "\n",
    "    # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = re.sub(BAD_SYMBOLS_RE,\"\",text)\n",
    "    \n",
    "    # ('\\s+') will match one or more whitespace characters, r - indicates raw string\n",
    "    text = re.sub(r'\\s+',\" \",text)\n",
    "\n",
    "    # delete stopwords from text\n",
    "    text = f'{join_symbol}'.join([i for i in text.split() if i not in STOPWORDS])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a test function to check helper function is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_prepare():\n",
    "    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\n",
    "    answers = [\"sql server equivalent excels choose function\", \n",
    "               \"free c++ memory vectorint arr\"]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if text_prepare(ex, ' ') != ans:\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "# Testing helper function\n",
    "print(test_text_prepare())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We tested that our helper function for preparing text is working correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can preprocess the titles using function 'text_prepare' and making sure that our data is more structured and the headers don't have bad symbols, stop words etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [text_prepare(x, \" \") for x in X_train]\n",
    "X_val = [text_prepare(x, \" \") for x in X_val]\n",
    "X_test = [text_prepare(x, \" \") for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [text_prepare(x, \",\") for x in y_train]\n",
    "y_val = [text_prepare(x, \",\") for x in y_val]\n",
    "#y_test = [text_prepare(x) for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draw stacked dotplot r', 'mysql select records datetime field less specified value', 'terminate windows phone 81 app', 'get current time specific country via jquery', 'configuring tomcat use ssl']\n",
      "['odbc_exec always fail', 'access base classes variable within child class', 'contenttype application json required rails', 'sessions sinatra used pass variable', 'getting error type json exist postgresql rake db migrate']\n",
      "['r', 'php,mysql', 'c#', 'javascript,jquery', 'java']\n",
      "['php,sql', 'javascript', 'rubyonrails,ruby', 'ruby,session', 'rubyonrails,ruby,json']\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])\n",
    "print(X_val[:5])\n",
    "\n",
    "print(y_train[:5])\n",
    "print(y_val[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will try to find the 3 most popular tags and 3 most popular words in train data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = Counter(chain.from_iterable([i.split(\" \") for i in X_train]))\n",
    "\n",
    "# Dictionary of all tags from train corpus with their counts\n",
    "tags_counts = Counter(chain.from_iterable([i.split(\",\") for i in y_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_most_common_words = sorted(words_counts.items(), key = lambda x: x[1], reverse = True)[:3]\n",
    "top_3_most_common_tags = sorted(tags_counts.items(), key = lambda x: x[1], reverse = True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('using', 8278), ('php', 5614), ('java', 5501)]\n",
      "[('javascript', 19078), ('c#', 19077), ('java', 18661)]\n"
     ]
    }
   ],
   "source": [
    "print(top_3_most_common_words)\n",
    "print(top_3_most_common_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After applying the sorting procedure, we see results of top 3 most common words/tags and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top three most popular words are: using, php, java\n",
      "Top three most popular tags are: javascript, c#, java\n"
     ]
    }
   ],
   "source": [
    "print(f\"Top three most popular words are: {', '.join(word for word, _ in top_3_most_common_words)}\")\n",
    "print(f\"Top three most popular tags are: {', '.join(tag for tag, _ in top_3_most_common_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming text to a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. Here we will try to use the two most common and simple approaches i.e. Bag of Words and TF-IDF ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One of the well-known approaches is a *bag-of-words* representation. To create this transformation, we perform the following steps:\n",
    "1. Find *N* most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n",
    "2. For each title in the corpora create a zero vector with the dimension equals to *N*.\n",
    "3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see a toy example. Imagine that we have *N* = 4 and the list of the most popular words is \n",
    "####     ['hi', 'you','me', 'are']\n",
    "\n",
    "#### Then we need to numerate them, for example, like this: \n",
    "\n",
    "####     {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    " \n",
    "#### And we have the text, which we want to transform to the vector:\n",
    " \n",
    "####     'hi how are you'\n",
    " \n",
    "#### For this text we create a corresponding zero vector \n",
    " \n",
    "####     [0, 0, 0, 0]\n",
    "     \n",
    "#### And interate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:\n",
    "\n",
    "####     'hi':  [1, 0, 0, 0]\n",
    "####     'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\n",
    "####     'are': [1, 0, 0, 1]\n",
    "####     'you': [1, 1, 0, 1]\n",
    "\n",
    "#### The resulting vector will be \n",
    " \n",
    "####     [1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will implement the encoding described above in the function 'my_bag_of_words' with the size of the dictionary - 5000. To find the most common words, we will use train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = 5000\n",
    "\n",
    "# most_common_words contain 5000 words in sorted order of frequency\n",
    "most_common_words = sorted(words_counts.items(), key = lambda x: x[1], reverse = True)[:DICT_SIZE] \n",
    "\n",
    "WORDS_TO_INDEX = {}\n",
    "INDEX_TO_WORDS = {}\n",
    "\n",
    "for i in range(0, DICT_SIZE):\n",
    "    \n",
    "    # most_common_words[i][0] means extracting ith word from the dictionary, words to index contain \n",
    "    # the index value of the word \n",
    "    WORDS_TO_INDEX[most_common_words[i][0]] = i   \n",
    "    \n",
    "    # index to word conatain the word correspond to the index \n",
    "    INDEX_TO_WORDS[i] = most_common_words[i][0]\n",
    "    \n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()\n",
    "\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        dict_size: size of the dictionary\n",
    "        \n",
    "        return a vector which is a bag-of-words representation of 'text'\n",
    "    \"\"\"\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    y = text.split(\" \")\n",
    "    for i in range(0,len(y)):\n",
    "        for key, value in words_to_index.items():\n",
    "            if y[i] == key:\n",
    "                #  result_vector[words_to_index[key]] contain the count of the presence of\n",
    "                #  word in the text\n",
    "                result_vector[words_to_index[key]] = result_vector[words_to_index[key]] + 1  \n",
    "    \n",
    "    # result vector is the vector of the size of the no of words taken as features having count\n",
    "    # of then in the text            \n",
    "    return result_vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a test function to check 'my_bag_of_words' function is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_my_bag_of_words():\n",
    "    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "    examples = ['hi how are you']\n",
    "    answers = [[1, 1, 0, 1]]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if (my_bag_of_words(ex, words_to_index, 4) != ans).any():\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_my_bag_of_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We tested that our function for bag of words representation is working correctly. So, now we can apply the implemented function to all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, we should note that our data might have lot of zero values and can cause problems with regards to space and time complexity in our current matrix (dense) representation. We can use an alternate data structure to represent our sparse data where the zero values can be ignored, thus storing the useful infomation efficiently. There are many types of such representations, however for simplicity we will use sklearn's csr matrix(sparse).\n",
    "\n",
    "#### Note: Performance on CSR/CSC is severly limited in performance terms by overhead of generating indices. Blocked CSR/CSC is a much better approach especially for SIMD machines and allows loop unrolling and vectorisation to vastly improve performance compared to vanilla CSC/CSR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (100000, 5000)\n",
      "X_val shape  (30000, 5000)\n",
      "X_test shape  (20000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_val shape ', X_val_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "#### The second approach extends the bag-of-words framework by taking into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space. \n",
    "#### We will implement function 'tfidf_features' using 'TfidfVectorizer' from 'scikit-learn'and use train data to train a vectorizer. We will filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, we will use bigrams along with unigrams in our vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "        X_train, X_val, X_test — samples        \n",
    "        return TF-IDF vectorized representation of each sample and vocabulary\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer with a proper parameters choice\n",
    "    # Fit the vectorizer on the train set\n",
    "    # Transform the train, test, and val sets and return the result\n",
    "    \n",
    "    tfidf_vectorizer =  TfidfVectorizer(min_df = 5, max_df = 0.9, \n",
    "                                        ngram_range =(1,2), token_pattern = '(\\S+)')\n",
    "    #  '(\\S+)' will match one or more whitespace characters\n",
    "    \n",
    "    X_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val = tfidf_vectorizer.transform(X_val)\n",
    "    X_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # tfidf_vectorizer.vocabulary_ returns dictionary of word, index\n",
    "    return X_train, X_val, X_test, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will test our 'tfidf_features' function by checking whether we have 'c++' or 'c#' in our vocabulary, as they are obviously important tokens in our tags prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\n",
    "tfidf_reversed_vocab = {i:word for word, i in tfidf_vocab.items()}\n",
    "\n",
    "print(\"c#\" in set(tfidf_reversed_vocab.values()))\n",
    "print(\"c++\" in set(tfidf_reversed_vocab.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12748)\t0.4309937630129157\n",
      "  (0, 14941)\t0.7126565202061851\n",
      "  (0, 4792)\t0.5535025387941576\n",
      "  (1, 4093)\t0.39639224964237335\n",
      "  (1, 14054)\t0.4089312040982416\n",
      "  (1, 10426)\t0.3621376616529093\n",
      "  (1, 17129)\t0.18110148646398525\n",
      "  (1, 14801)\t0.29994308533196384\n",
      "  (1, 9077)\t0.3287166709387216\n",
      "  (1, 5815)\t0.2382368446529078\n",
      "  (1, 4089)\t0.2692803496632626\n",
      "  (1, 13008)\t0.2975359437533551\n",
      "  (1, 14019)\t0.22859508855051242\n",
      "  (1, 10394)\t0.20888863770024907\n",
      "X_test_tfidf  (20000, 18300)\n",
      "X_val_tfidf  (30000, 18300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[:2])\n",
    "print('X_test_tfidf ', X_test_tfidf.shape) \n",
    "print('X_val_tfidf ',X_val_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLabel classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here each example can have multiple tags, so to deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose it is convenient to use MultiLabelBinarizer from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we will create a list of unique tags from our training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [set(i.split(',')) for i in y_train]\n",
    "y_val = [set(i.split(',')) for i in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# changes the y_train, y_val in feature form like all clases with 0,1 value\n",
    "y_train = mlb.fit_transform(y_train) \n",
    "y_val = mlb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ajax', 'algorithm', 'android', 'angularjs', 'apache', 'arrays',\n",
       "       'aspnet', 'aspnetmvc', 'c', 'c#', 'c++', 'class', 'cocoatouch',\n",
       "       'codeigniter', 'css', 'csv', 'database', 'date', 'datetime',\n",
       "       'django', 'dom', 'eclipse', 'entityframework', 'excel', 'facebook',\n",
       "       'file', 'forms', 'function', 'generics', 'googlemaps', 'hibernate',\n",
       "       'html', 'html5', 'image', 'ios', 'iphone', 'java', 'javascript',\n",
       "       'jquery', 'json', 'jsp', 'laravel', 'linq', 'linux', 'list',\n",
       "       'loops', 'maven', 'mongodb', 'multithreading', 'mysql', 'net',\n",
       "       'nodejs', 'numpy', 'objectivec', 'oop', 'opencv', 'osx', 'pandas',\n",
       "       'parsing', 'performance', 'php', 'pointers', 'python', 'python27',\n",
       "       'python3x', 'qt', 'r', 'regex', 'rest', 'ruby', 'rubyonrails',\n",
       "       'rubyonrails3', 'selenium', 'servlets', 'session', 'sockets',\n",
       "       'sorting', 'spring', 'springmvc', 'sql', 'sqlserver', 'string',\n",
       "       'swift', 'swing', 'twitterbootstrap', 'uitableview', 'unittesting',\n",
       "       'validation', 'vbnet', 'visualstudio', 'visualstudio2010', 'wcf',\n",
       "       'webservices', 'windows', 'winforms', 'wordpress', 'wpf', 'xaml',\n",
       "       'xcode', 'xml'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the names of unique tags in our list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will implement the function 'train_classifier' for training a classifier. Here we will use One-vs-Rest approach, which is implemented in OneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a basic classifier, use LogisticRegression. It is one of the simplest methods, but often it performs good enough in text classification tasks. \n",
    "#### Note: It might take some time, because a number of classifiers to train is large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "      X_train, y_train — training data\n",
    "      \n",
    "      return: trained classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create and fit LogisticRegression wraped into OneVsRestClassifier.\n",
    "    model = OneVsRestClassifier(LogisticRegression(max_iter = 200)).fit(X_train,y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will train the classifiers for different data transformations i.e. 'bag-of-words' and 'tf-idf'.\n",
    "#### Note: If you receive a convergence warning, please set parameter 'max_iter' in LogisticRegression to a larger value (the default is 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_mybag = train_classifier(X_train_mybag, y_train)\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can create predictions for the data. We will do two types of predictions: labels and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag) \n",
    "y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n",
    "\n",
    "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
    "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now check how our classifier works using labels prediction, i.e. for bag of words and TF-IDF by taking few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\todbc_exec always fail\n",
      "True labels:\tphp,sql\n",
      "Predicted labels:\t\n",
      "\n",
      "\n",
      "Title:\taccess base classes variable within child class\n",
      "True labels:\tjavascript\n",
      "Predicted labels:\t\n",
      "\n",
      "\n",
      "Title:\tcontenttype application json required rails\n",
      "True labels:\truby,rubyonrails\n",
      "Predicted labels:\trubyonrails\n",
      "\n",
      "\n",
      "Title:\tsessions sinatra used pass variable\n",
      "True labels:\truby,session\n",
      "Predicted labels:\truby\n",
      "\n",
      "\n",
      "Title:\tgetting error type json exist postgresql rake db migrate\n",
      "True labels:\tjson,ruby,rubyonrails\n",
      "Predicted labels:\tjson,rubyonrails\n",
      "\n",
      "\n",
      "Title:\tlibrary found\n",
      "True labels:\tc++,ios,iphone,xcode\n",
      "Predicted labels:\t\n",
      "\n",
      "\n",
      "Title:\tcsproj file programmatic adding deleting files\n",
      "True labels:\tc#\n",
      "Predicted labels:\t\n",
      "\n",
      "\n",
      "Title:\ttypeerror makedirs got unexpected keyword argument exists_ok\n",
      "True labels:\tdjango,python\n",
      "Predicted labels:\tpython\n",
      "\n",
      "\n",
      "Title:\tpan div using jquery\n",
      "True labels:\thtml,javascript,jquery\n",
      "Predicted labels:\tjavascript,jquery\n",
      "\n",
      "\n",
      "Title:\thibernate intermediate advanced tutorials\n",
      "True labels:\thibernate,java\n",
      "Predicted labels:\thibernate,java\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_val_pred_inversed = mlb.inverse_transform(y_val_predicted_labels_mybag)\n",
    "y_val_inversed = mlb.inverse_transform(y_val)\n",
    "\n",
    "for i in range(10):\n",
    "    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n",
    "        X_val[i],\n",
    "        ','.join(y_val_inversed[i]),\n",
    "        ','.join(y_val_pred_inversed[i])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We checked how our bag of words classifier works. Now we can do the same for TF-IDF classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\todbc_exec always fail\n",
      "True labels:\tphp,sql\n",
      "Predicted labels:\t\n",
      "\n",
      "\n",
      "Title:\taccess base classes variable within child class\n",
      "True labels:\tjavascript\n",
      "Predicted labels:\t\n",
      "\n",
      "\n",
      "Title:\tcontenttype application json required rails\n",
      "True labels:\truby,rubyonrails\n",
      "Predicted labels:\tjson,rubyonrails\n",
      "\n",
      "\n",
      "Title:\tsessions sinatra used pass variable\n",
      "True labels:\truby,session\n",
      "Predicted labels:\t\n",
      "\n",
      "\n",
      "Title:\tgetting error type json exist postgresql rake db migrate\n",
      "True labels:\tjson,ruby,rubyonrails\n",
      "Predicted labels:\trubyonrails\n",
      "\n",
      "\n",
      "Title:\tlibrary found\n",
      "True labels:\tc++,ios,iphone,xcode\n",
      "Predicted labels:\t\n",
      "\n",
      "\n",
      "Title:\tcsproj file programmatic adding deleting files\n",
      "True labels:\tc#\n",
      "Predicted labels:\t\n",
      "\n",
      "\n",
      "Title:\ttypeerror makedirs got unexpected keyword argument exists_ok\n",
      "True labels:\tdjango,python\n",
      "Predicted labels:\tpython\n",
      "\n",
      "\n",
      "Title:\tpan div using jquery\n",
      "True labels:\thtml,javascript,jquery\n",
      "Predicted labels:\tjavascript,jquery\n",
      "\n",
      "\n",
      "Title:\thibernate intermediate advanced tutorials\n",
      "True labels:\thibernate,java\n",
      "Predicted labels:\thibernate,java\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_val_pred_inversed = mlb.inverse_transform(y_val_predicted_labels_tfidf)\n",
    "y_val_inversed = mlb.inverse_transform(y_val)\n",
    "\n",
    "for i in range(10):\n",
    "    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n",
    "        X_val[i],\n",
    "        ','.join(y_val_inversed[i]),\n",
    "        ','.join(y_val_pred_inversed[i])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we would need to compare the results of different predictions, e.g. to see which classifier works better (bag of words or TF-IDF) or to try different regularization techniques in logistic regression or a different model like SVM, Naive Bayes etc. For all these experiments, we need to setup evaluation procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    " \n",
    "#### To evaluate the results we will use several classification metrics:\n",
    "- Accuracy\n",
    "- F1-score\n",
    "- Area under ROC-curve\n",
    "- Area under precision-recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will implement the function 'print_evaluation_scores' which calculates\n",
    "* accuracy\n",
    "* F1-score macro/micro/weighted\n",
    "* Precision macro/micro/weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words\n",
      "                      Values\n",
      "Names                       \n",
      "accuracy            0.357800\n",
      "f1_score_macro      0.504803\n",
      "f1_score_micro      0.671006\n",
      "f1_score_weighted   0.648667\n",
      "precision_macro     0.344404\n",
      "precision_micro     0.481164\n",
      "precision_weighted  0.510749 \n",
      "\n",
      "Tfidf\n",
      "                      Values\n",
      "Names                       \n",
      "accuracy            0.333900\n",
      "f1_score_macro      0.445477\n",
      "f1_score_micro      0.641718\n",
      "f1_score_weighted   0.614248\n",
      "precision_macro     0.301817\n",
      "precision_micro     0.456897\n",
      "precision_weighted  0.485003 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_evaluation_scores(y_val, predicted):\n",
    "    \n",
    "    accuracy = accuracy_score(y_val, predicted)\n",
    "    f1_score_macro = f1_score(y_val, predicted, average = 'macro')\n",
    "    f1_score_micro = f1_score(y_val, predicted, average = 'micro')\n",
    "    f1_score_weighted = f1_score(y_val, predicted, average = 'weighted')\n",
    "    precision_macro = average_precision_score(y_val, predicted, average = 'macro')\n",
    "    precision_micro = average_precision_score(y_val, predicted, average = 'micro')\n",
    "    precision_weighted = average_precision_score(y_val, predicted, average = 'weighted')\n",
    "\n",
    "    scores_names = ['accuracy', 'f1_score_macro', 'f1_score_micro', 'f1_score_weighted', 'precision_macro',\n",
    "          'precision_micro', 'precision_weighted']\n",
    "    scores_values = [accuracy, f1_score_macro, f1_score_micro, f1_score_weighted, precision_macro,\n",
    "          precision_micro, precision_weighted]\n",
    "    print(pd.DataFrame({'Names':scores_names, 'Values': scores_values}).set_index('Names'),'\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "print('Bag-of-words')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_mybag)\n",
    "print('Tfidf')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "#### Once we have the evaluation set up, we can experiment a bit with training your classifiers. We will use F1-score weighted as an evaluation metric and try the following:\n",
    "- compare the quality of the bag-of-words and TF-IDF approaches and choose one of them.\n",
    "- for the chosen one, try *L1* and *L2*-regularization techniques in Logistic Regression with different coefficients (e.g. C equal to 0.1, 1, 10, 100).\n",
    "- We could try to do further improvements of the preprocessing\n",
    "- We could try some other model like SVM, Naive Bayes, CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the most important features\n",
    "\n",
    "#### Finally, we can also look at the features (words or n-grams) that are used with the largest weigths in our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
